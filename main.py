import json
from schema import RELATION_TYPES
from rules import clean_output
from prompt_template import PROMPT_TEMPLATE
import os
from openai import OpenAI
from tqdm import tqdm   # âœ… æ–°å¢ï¼šè¿›åº¦æ¡åº“
from docx import Document
import requests, os
from concurrent.futures import ThreadPoolExecutor, as_completed
from retriever.retrieve_context import retrieve_context


# os.environ["HTTP_PROXY"] = "http://127.0.0.1:7890"
# os.environ["HTTPS_PROXY"] = "http://127.0.0.1:7890"

# from openai import OpenAI


# class OpenAILLM:
#     def __init__(self):
#         self.client = OpenAI(api_key="""
#     def generate(self, prompt):
#         response = self.client.chat.completions.create(
#             model="gpt-4o-mini",   # ä¹Ÿå¯ä»¥æ¢æˆ "gpt-3.5-turbo"
#             messages=[
#                 {"role": "system", "content": "ä½ æ˜¯ä¸€ä¸ªä¿¡æ¯æŠ½å–åŠ©æ‰‹"},
#                 {"role": "user", "content": prompt}
#             ]
#         )
#         return response.choices[0].message.content


# # =======================
# # å•æ¡æ–‡æœ¬å¤„ç†
# # =======================
# def weak_label(text, llm_client):
#     prompt = PROMPT_TEMPLATE.format(input_text=text)
#     response = llm_client.generate(prompt)
#     try:
#         data = json.loads(response)
#         return clean_output(data, RELATION_TYPES)
#     except Exception:
#         return {"entities": [], "relations": []}


# # =======================
# # å¹¶å‘ pipeline
# # =======================
# def pipeline(texts, llm_client, max_workers=5):
#     results = []
#     with ThreadPoolExecutor(max_workers=max_workers) as executor:
#         future_to_text = {executor.submit(weak_label, t, llm_client): t for t in texts}
#         for future in tqdm(as_completed(future_to_text), total=len(texts), desc="Processing texts", unit="text"):
#             try:
#                 results.append(future.result())
#             except Exception as e:
#                 print("âŒ å‡ºé”™:", e)
#                 results.append({"entities": [], "relations": []})
#     return results


# # =======================
# # ä¸»ç¨‹åº
# # =======================
# if __name__ == "__main__":
#     doc = Document("C:/PHD/2025/08/food_nutrition.docx")
#     texts = [para.text.strip() for para in doc.paragraphs if para.text.strip()]

#     llm_client = OpenAILLM()
#     results = pipeline(texts, llm_client, max_workers=5)  # âœ… å¹¶å‘ 5 ä¸ªä»»åŠ¡

#     with open("spert_dataset.jsonl", "w", encoding="utf-8") as f:
#         for item in results:
#             f.write(json.dumps(item, ensure_ascii=False) + "\n")

#     print("âœ… spert_dataset.jsonl å·²ç”Ÿæˆ")

import json
from schema import RELATION_TYPES
from rules import clean_output
from prompt_template import PROMPT_TEMPLATE
import os
from openai import OpenAI
from tqdm import tqdm
from docx import Document
import re
from concurrent.futures import ThreadPoolExecutor, as_completed


# ==============================
# ä»£ç†è®¾ç½®ï¼ˆå¦‚æœ‰ï¼‰
# ==============================
os.environ["HTTP_PROXY"] = "http://127.0.0.1:7890"
os.environ["HTTPS_PROXY"] = "http://127.0.0.1:7890"


# ==============================
# GPT å°è£…
# ==============================
class OpenAILLM:
    def __init__(self):
        self.client = OpenAI(api_key="")
    def generate(self, prompt):
        response = self.client.chat.completions.create(
            model="gpt-4o-mini",
            messages=[
                {"role": "system", "content": "ä½ æ˜¯ä¸€ä¸ªä¿¡æ¯æŠ½å–åŠ©æ‰‹"},
                {"role": "user", "content": prompt}
            ]
        )
        return response.choices[0].message.content


# ==============================
# åˆ¤æ–­æ ‡é¢˜ç±»å‹
# ==============================
def is_main_chapter(text):
    return bool(re.match(r"^ç¬¬[ä¸€äºŒä¸‰å››äº”å…­ä¸ƒå…«ä¹åç™¾]+ç« ", text))

def is_subsection(text):
    return bool(re.match(r"^\d+\.\d+(\.\d+)?", text))  # æ”¯æŒ 1.2 æˆ– 1.2.1


# ==============================
# ä» Word ä¸­æå–ç»“æ„åŒ–æ®µè½
# ==============================
def extract_structured_paragraphs(doc_path):
    doc = Document(doc_path)
    paragraphs = [para.text.strip() for para in doc.paragraphs if para.text.strip()]

    current_main = ""
    current_sub = ""
    current_text = ""
    structured_samples = []

    for para in paragraphs:
        if is_main_chapter(para):
            current_main = para
            continue
        elif is_subsection(para):
            if current_text:
                structured_samples.append({
                    "main_section": current_main,
                    "subsection": current_sub,
                    "text": current_text.strip()
                })
            current_sub = para
            current_text = ""
        else:
            current_text += para + " "

    if current_text:
        structured_samples.append({
            "main_section": current_main,
            "subsection": current_sub,
            "text": current_text.strip()
        })

    return structured_samples


# ==============================
# è°ƒç”¨ GPT å¼±æ ‡æ³¨
# ==============================
def weak_label(sample, llm_client):
    context = retrieve_context(sample["text"])  # ğŸ” ä»å‘é‡åº“æ£€ç´¢ç›¸å…³èƒŒæ™¯
    prompt = PROMPT_TEMPLATE.format(context=context, input_text=sample["text"])
    response = llm_client.generate(prompt)
    try:
        data = json.loads(response)
        result = clean_output(data, RELATION_TYPES)
        result["main_section"] = sample["main_section"]
        result["subsection"] = sample["subsection"]
        return result
    except Exception as e:
        print("âŒ JSON è§£æå¤±è´¥:", e)
        return {
            "main_section": sample["main_section"],
            "subsection": sample["subsection"],
            "entities": [],
            "relations": []
        }


# ==============================
# å¹¶å‘ pipeline
# ==============================
def pipeline(samples, llm_client, max_workers=5):
    results = []
    with ThreadPoolExecutor(max_workers=max_workers) as executor:
        futures = {
            executor.submit(weak_label, sample, llm_client): sample for sample in samples
        }
        for future in tqdm(as_completed(futures), total=len(samples), desc="Processing sections", unit="section"):
            try:
                results.append(future.result())
            except Exception as e:
                sample = futures[future]
                print("âŒ æ‰§è¡Œå¤±è´¥:", e)
                results.append({
                    "main_section": sample["main_section"],
                    "subsection": sample["subsection"],
                    "entities": [],
                    "relations": []
                })
    return results


# ==============================
# ä¸»ç¨‹åº
# ==============================
if __name__ == "__main__":
    doc_path = "C:/PHD/2025/08/food_nutrition.docx"  # âœ… æ›¿æ¢ä¸ºä½ çš„ docx è·¯å¾„

    print("ğŸ“– æ­£åœ¨ç»“æ„åŒ–æå–ç« èŠ‚æ®µè½...")
    samples = extract_structured_paragraphs(doc_path)
    print(f"âœ… å…±æå– {len(samples)} æ¡ç»“æ„åŒ–æ®µè½")

    llm_client = OpenAILLM()
    results = pipeline(samples, llm_client, max_workers=5)

    print("ğŸ’¾ æ­£åœ¨å†™å…¥ spert_dataset.jsonl...")
    with open("spert_dataset.jsonl", "w", encoding="utf-8") as f:
        for item in results:
            # åˆ é™¤ç»“æ„å­—æ®µ
            item.pop("main_section", None)
            item.pop("subsection", None)
            f.write(json.dumps(item, ensure_ascii=False) + "\n")


    print("âœ… æ•°æ®ç”Ÿæˆå®Œæ¯•ï¼Œæ–‡ä»¶ä¸ºï¼šspert_dataset.jsonl")
